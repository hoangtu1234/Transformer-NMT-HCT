{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4Raxs63BecbJ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import math"]},{"cell_type":"code","source":["class InputEmbeddings(nn.Module):\n","  def __init__(self,d_model:int, vocab_size : int ):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.vocab_size = vocab_size\n","    self.embedding = nn.Embedding(vocab_size, d_model)\n","  def forward(self, x):\n","    return self.embedding(x)*math.sqrt(self.d_model)\n"],"metadata":{"id":"NfVuicd-fcXL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","  def __init__(self, d_model: int, seq_len: int, dropout : float):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.seq_len = seq_len\n","    self.dropout = nn.Dropout(dropout)\n","    # Tao ma tran shape (seq_len, d_model)\n","    pe = torch.zeros(seq_len, d_model)\n","    # Tao mang chieu dai cua chuoi\n","    position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1)\n","    div_term = torch.exp(torch.arange(0,d_model, 2).float() * (-math.log(10000.0)/d_model))\n","    pe[:,0::2] = torch.sin(position * div_term)\n","    pe[:,1::2] = torch.cos(position * div_term)\n","\n","    pe = pe.unsqueeze(0)\n","\n","    self.register_buffer('pe', pe)\n","  def forward(self,x1):\n","    x1 = x1 + (self.pe[: , :x1.shape[1], :]).requires_grad_(False)\n","    return self.dropout(x1)\n","\n"],"metadata":{"id":"B0eLsDPrglYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LayerNormalization(nn.Module):\n","  def __init__(self,eps :float = 10**-6) -> None:\n","    super().__init__()\n","    self.eps = eps\n","    self.alpha = nn.Parameter(torch.ones(1))\n","    self.bias = nn.Parameter(torch.ones(1))\n","\n","  def forward(self,x):\n","    mean = x.mean(dim = -1, keepdim = True)\n","    std = x.std(dim = -1, keepdim = True)\n","    return self.alpha*(x - mean)/(self.eps + std) + self.bias"],"metadata":{"id":"jS8GczaZfcLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForwardBlock(nn.Module):\n","  def __init__(self, d_model : int, dff : int, dropout : float) -> None:\n","    super().__init__()\n","    self.linear_1 = nn.Linear(d_model, dff)\n","    self.dropout = nn.Dropout(dropout)\n","    self.linear_2 = nn.Linear(dff,d_model)\n","  def forward(self,x):\n","    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n","\n"],"metadata":{"id":"a-E5kotwfcFg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttentionBlock(nn.Module):\n","  def __init__(self, d_model: int, h:int, dropout:float) -> None:\n","    super().__init__()\n","    self.d_model = d_model\n","    self.h = h\n","    self.dk = d_model // h\n","    self.w_q = nn.Linear(d_model,d_model)\n","    self.w_k = nn.Linear(d_model,d_model)\n","    self.w_v = nn.Linear(d_model,d_model)\n","    self.w_o = nn.Linear(d_model,d_model)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  @staticmethod\n","  def attention(query, key, value,mask, dropout: nn.Dropout):\n","    d_k = query.shape[-1]\n","    attention_scores = (query@key.transpose(-2,-1)) /math.sqrt(d_k)\n","    if mask is not None:\n","      attention_scores.masked_fill_(mask ==0, -1e9)\n","    attension_scores = attention_scores.softmax(dim = -1)\n","    if dropout is not None:\n","      attention_scores = dropout(attention_scores)\n","    return (attention_scores @ value), attention_scores\n","\n","\n","\n","  def forward(self,q,k,v, mask):\n","    query = self.w_q(q)\n","    key = self.w_k(k)\n","    value = self.w_v(v)\n","\n","    query = query.view(query.shape[0],query.shape[1],self.h, self.dk).transpose(1,2)\n","    key = key.view(key.shape[0],key.shape[1],self.h,self.dk).transpose(1,2)\n","    value = value.view(value.shape[0],value.shape[1],self.h,self.dk).transpose(1,2)\n","\n","    x, self.attention_scoress = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n","\n","    x = x.transpose(1,2).reshape(query.shape[0],-1,self.d_model)\n","    return self.w_o(x)\n","\n","class ResidualConnection(nn.Module):\n","  def __init__(self, dropout:float) -> None:\n","    super().__init__()\n","    self.dropout = nn.Dropout(dropout)\n","    self.norm = LayerNormalization()\n","\n","\n","  def forward(self, x, sub_layer):\n","    return x + self.dropout(sub_layer(self.norm(x)))\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"nKbL394bVGbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderBlock(nn.Module):\n","  def __init__(self,self_attention_block : MultiHeadAttentionBlock, feed_forward_block : FeedForwardBlock, dropout:float) -> None:\n","    super().__init__()\n","    self.self_attention_block = self_attention_block\n","    self.feed_forward_block = feed_forward_block\n","    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n","\n","  def forward(self, x , src_mask):\n","    x = x + self.residual_connections[0](x,lambda x : self.self_attention_block(x,x,x,src_mask))\n","    x = self.residual_connections[1](x, self.feed_forward_block)\n","    return x\n","class Encoder(nn.Module):\n","  def __init__(self, layers :nn.ModuleList) -> None:\n","    super().__init__()\n","    self.layers = layers\n","    self.norm = LayerNormalization()\n","\n","  def forward(self, x, mask):\n","    for layer in self.layers:\n","      x = layer(x, mask)\n","    return self.norm(x)\n","\n","\n"],"metadata":{"id":"-n4Mt1P4VF_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DecoderBlock(nn.Module):\n","  def __init__(self, self_attention_block:MultiHeadAttentionBlock,cross_attention_block:MultiHeadAttentionBlock, feed_forward_block:FeedForwardBlock, dropout:float) -> None:\n","    super().__init__()\n","    self.self_attention_block = self_attention_block\n","    self.cross_attention_block = cross_attention_block\n","    self.feed_forward_block = feed_forward_block\n","    self.dropout = dropout\n","    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n","  def forward(self, x, encoder_output, src_mask, tgt_mask):\n","    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x,tgt_mask))\n","    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n","    x = self.residual_connections[2](x, self.feed_forward_block)\n","    return x\n","\n","class Decoder(nn.Module):\n","  def __init__(self, layers : nn.ModuleList) -> None:\n","    super().__init__()\n","    self.layers = layers\n","    self.norm = LayerNormalization()\n","  def forward(self, x , encoder_output, src_mask, tgt_mask ):\n","    for layer in self.layers:\n","      x = layer(x, encoder_output, src_mask, tgt_mask)\n","      return self.norm(x)\n","\n"],"metadata":{"id":"Tp3F9LmkwMYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ProjectionLayer(nn.Module):\n","  def __init__(self, d_model: int, vocab_size:int) -> None:\n","    super().__init__()\n","    self.project_matrix = nn.Linear(d_model, vocab_size)\n","\n","  def forward(self, x):\n","    return torch.log_softmax(self.project_matrix(x), dim = -1)\n","\n"],"metadata":{"id":"MBi4W8CNz0Zm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","  def __init__(self, encoder : Encoder, decoder :Decoder, src_embed : InputEmbeddings, tgt_embed : InputEmbeddings,src_pos : PositionalEncoding, tgt_pos :PositionalEncoding, projection_layer : ProjectionLayer) -> None:\n","    super().__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.src_embed = src_embed\n","    self.tgt_embed = tgt_embed\n","    self.src_pos = src_pos\n","    self.tgt_pos = tgt_pos\n","    self.projection_layer = projection_layer\n","  def encode(self, src, src_mask):\n","    src = self.src_embed(src)\n","    src = self.src_pos(src)\n","    return self.encoder(src, src_mask)\n","  def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n","    tgt = self.tgt_embed(tgt)\n","    tgt = self.tgt_pos(tgt)\n","    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n","  def project(self, x):\n","    return self.projection_layer(x)\n","\n","\n","\n","\n","\n"],"metadata":{"id":"7vNgSINV0pi4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_transformer(src_vocab_size : int, tgt_vocab_size : int, src_seq_len:int, tgt_seq_len:int,d_model:int = 512,N:int = 6, h:int = 8, dropout:float = 0.1, d_ff:int =2048 ):\n","  src_embed = InputEmbeddings(d_model, src_vocab_size)\n","  tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n","\n","  src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n","  tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n","\n","\n","  encoder_blocks = []\n","\n","  for _ in range(N):\n","    self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n","    feed_forward_block = FeedForwardBlock(d_model,d_ff,dropout)\n","    encoder_block = EncoderBlock(self_attention_block, feed_forward_block,dropout)\n","    encoder_blocks.append(encoder_block)\n","  decoder_blocks = []\n","  for _ in range(N):\n","    self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n","    cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n","    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n","    decoder_block = DecoderBlock(self_attention_block, cross_attention_block, feed_forward_block, dropout)\n","    decoder_blocks.append(decoder_block)\n","\n","  encoder = Encoder(nn.ModuleList(encoder_blocks))\n","  decoder = Decoder(nn.ModuleList(decoder_blocks))\n","\n","\n","  projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n","\n","  transformer = Transformer(encoder,decoder,src_embed,tgt_embed,src_pos, tgt_pos,projection_layer)\n","\n","  for p in transformer.parameters():\n","    if p.dim() > 1:\n","      nn.init.xavier_uniform_(p)\n","  return transformer\n","\n","\n"],"metadata":{"id":"obr88gq_3Z7S"},"execution_count":null,"outputs":[]}]}