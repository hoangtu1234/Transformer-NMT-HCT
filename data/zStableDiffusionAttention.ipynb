{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqlknpkupYsU1Gb4YvRxdL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MLt_fUb6c5f2"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","import math"]},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","  def __init__(self, n_heads : int, d_embed :int, in_proj_bias = True, out_proj_bias = True):\n","    super().__init__()\n","    self.n_heads = n_heads\n","    self.d_head = d_embed // n_heads\n","    self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias = in_proj_bias)\n","    self.out_proj = nn.Linear(d_embed, d_embed, bias = out_proj_bias)\n","  def forward(self, x, causal_mask = False):\n","    # x: # (Batch_Size, Seq_Len, Dim)\n","    batch_size, sequence_length, d_embed = x.shape\n","    # (Batch_Size, Seq_Len, H, Dim / H)\n","    multihead_shape = (batch_size, self.n_heads, sequence_length, self.d_head)\n","    # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n","    q, k, v = self.in_proj(x).chunk(3, dim = -1)\n","    # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, H, Seq_Len, Dim / H)\n","    q = q.contiguous().view(*multihead_shape)\n","    # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, H, Seq_Len, Dim / H)\n","    k = k.contiguous().view(*multihead_shape)\n","    # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, H, Seq_Len, Dim / H)\n","    v = v.contiguous().view(*multihead_shape)\n","\n","    # (Batch_Size, H, Seq_Len, Dim / H) @ (Batch_Size, H, Dim / H, Seq_Len) -> (Batch_Size, H, SeqLen, Seq_Len)\n","    attention_weight = q @ k.transpose(-1,-2)\n","\n","    if casual_mask:\n","      # masked uper triangle matrix of \"1\" with diagonal location = 1\n","      mask = torch.ones_like(attention_weight).triu(1)\n","      # fill 0 value with negative inf\n","      weight.mask_fill_(mask == 0, -torch.inf)\n","    # Apply soft_max with the last dimension or the second last dimension\n","    weight = F.softmax(attention_weight, dim = -1)\n","    # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n","    output = (weight @ v).transpose(1,2)\n","    # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n","    output = output.contiguous().view(batch_size, sequence_length, d_embed)\n","    # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n","    return self.out_proj(output)"],"metadata":{"id":"AKW-Gh4CdINU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CrossAttention(nn.Module):\n","  def __init__(self, n_heads : int, d_embed : int, d_cross : int, in_proj_bias = True, out_proj_bias = True):\n","    super().__init__()\n","    self.n_heads = n_heads\n","    self.d_head = d_embed // n_heads\n","    self.querry_proj = nn.Linear(d_embed, d_embed, bias = in_proj_bias)\n","    self.key_proj = nn.Linear(d_cross, d_embed, bias = in_proj_bias)\n","    self.value_proj = nn.Linear(d_cross, d_embed, bias = in_proj_bias)\n","    self.out_proj = nn.Linear(d_embed, d_embed, bias = out_proj)\n","\n","  def forward(self, x, y):\n","    # x (latent): # (Batch_Size, Seq_Len_Q, Dim_Q)\n","    # y (context): # (Batch_Size, Seq_Len_KV, Dim_KV) = (Batch_Size, 77, 768)\n","    batch_size, sequence_length, d_embed = x.shape\n","    # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n","    query = self.querry_proj(x)\n","    # (Batch_Size, Seq_Len_KV, dim_KV) -> (Batch_Size, Seq_Len_KV, dim_Q )\n","    key = self.key_proj(y)\n","    # (Batch_Size, Seq_Len_KV, dim_KV) -> (Batch_Size, Seq_Len_KV, dim_Q )\n","    value = self.value_proj(y)\n","\n","    multihead_shape = (batch_size, -1, self.n_heads, self.d_head)\n","    # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n","    query = query.contigous().view(*multihead_shape).transpose(1,2)\n","    # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n","    key = key.contiguous().view(*multihead_shape).transpose(1,2)\n","    #(Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n","    value = value.contiguous().view(*multihead_shape).transpose(1,2)\n","\n","    weight = query @ key.transpose(-1,-2)\n","    weight = weight / math.sqrt(self.d_head)\n","    weight = F.softmax(weight, dim = -1)\n","    output = weight @ value\n","    output = output.transpose(1,2).contiguous().view(batch_size, sequence_length, d_embed)\n","    return self.out_proj(output)\n",""],"metadata":{"id":"2cQwaWRHtAmN"},"execution_count":null,"outputs":[]}]}